{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepKLM: A Library for Language Experiment using a Deep Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "To set up, run the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash ./scripts/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "from scripts.surprisal import bert_token_surprisal, bert_sentence_surprisal, confusion_score, confusion_score_batch\n",
    "from scripts.visualization import attention_heatmap, visualize_attention_head\n",
    "from scripts.boxplot_creator import draw_box_plot\n",
    "from scripts.barplot_creator import draw_bar_plot\n",
    "\n",
    "from sys import platform\n",
    "from os import path\n",
    "from torch import device\n",
    "from transformers import AdamW, BertConfig, BertModel, BertTokenizer, BertForMaskedLM\n",
    "from bertviz_lin.pytorch_pretrained_bert import BertForTokenClassification\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == \"linux\" or platform == \"linux2\": \n",
    "    flist = fm.get_fontconfig_fonts()\n",
    "    available_fonts = [fm.FontProperties(fname=fname).get_name() for fname in flist]\n",
    "    if 'NanumGothic' in available_fonts:\n",
    "        plt.rcParams['font.family'] = 'NanumGothic'\n",
    "    else:\n",
    "        print(\"Font NanumGothic was not found... Try installing a font\")\n",
    "        !apt-get update -qq\n",
    "        !apt-get install fonts-nanum* -qq\n",
    "        print(\"Installed the font!\")\n",
    "        fm._rebuild()\n",
    "        print(\"=================IMPORTANT==============================\")\n",
    "        print(\"If on Colab, RESTART THE RUNTIME to apply the font.\")\n",
    "elif platform == \"darwin\":\n",
    "    plt.rcParams['font.family'] = 'AppleGothic' \n",
    "elif platform == \"win32\":\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "else:\n",
    "    print(\"User platform could not be identified. Korean characters may not be shown correctly when visualizing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English\n",
    "\n",
    "Load BERT(Large, Uncased) by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_model_eng = BertForMaskedLM.from_pretrained('bert-large-uncased', output_attentions=True)\n",
    "classification_model_eng = BertForTokenClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "tokenizer_eng = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korean\n",
    "Load KR-BERT by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelpath= \"./KR-BERT/krbert_pytorch/pretrained/pytorch_model_char16424_ranked.bin\"\n",
    "config = BertConfig.from_json_file(\"./KR-BERT/krbert_pytorch/pretrained/bert_config_char16424.json\")\n",
    "config.output_attentions = True\n",
    "tokenizer_kr = BertTokenizer.from_pretrained('./KR-BERT/krbert_pytorch/pretrained/vocab_snu_char16424.txt', do_lower_case=False)\n",
    "mask_model_kr = BertForMaskedLM.from_pretrained(modelpath,config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ETRI KorBert if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if path.exists(\"KorBERT\"):\n",
    "  sys.path.insert(1, \"./KorBERT/001_bert_morp_pytorch/src_tokenizer\")\n",
    "  import tokenization_morp\n",
    "  \"\"\"IN CASE OF ImportError:\n",
    "  1. Go to the src_tokenizer.py in KorBERT\n",
    "  2. Go to line 32 (from .file_utils import cached_path)\n",
    "  3. Change the line to the following\n",
    "    from pytorch_pretrained_bert.file_utils import cached_path\n",
    "  4. Enjoy :)\n",
    "  \"\"\"\n",
    "\n",
    "  korbert_path = \"./KorBERT/001_bert_morp_pytorch/\"\n",
    "  modelpath= korbert_path + \"pytorch_model.bin\"\n",
    "  config = BertConfig.from_json_file(korbert_path + \"bert_config.json\")\n",
    "  mask_model_etri=BertForMaskedLM.from_pretrained(modelpath,config=config)\n",
    "  tokenizer_etri = tokenization_morp.BertTokenizer.from_pretrained(korbert_path + \"vocab.korean_morp.list\")\n",
    "else:\n",
    "  print(\"KorBERT not found. Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiement (Single Factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to do it\n",
    "    * [MASK] a common token -- here, love and loves\n",
    "    * set tokens to be input as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Each of the presentators [MASK] five minutes for their talk.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bert_token_surprisal(text, [\"have\", \"has\"], mask_model_eng, tokenizer_eng, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiement (Double Factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to do it\n",
    "    * [MASK] a common token -- here, love and loves\n",
    "    * set tokens to be input as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "John [MASK] Mary.\n",
    "I [MASK] Mary.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bert_token_surprisal(text, [\"love\", \"loves\"], mask_model_eng, tokenizer_eng, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiement (Triple Factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to do it\n",
    "    * [MASK] a common token -- here, love and loves\n",
    "    * set tokens to be input as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "John hates the girl that [MASK] me.\n",
    "John hates the girl who [MASK] me.\n",
    "John hates the girls that [MASK] me.\n",
    "John hates the girls who [MASK] me.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bert_token_surprisal(text, [\"love\", \"loves\"], mask_model_eng, tokenizer_eng, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cautions\n",
    "\n",
    "- Pairs may not be minimally different to a computer\n",
    "    - e.g. There is a book/an apple.\n",
    "        - while this can be argued to be minimally different at syntactic level as an\\~a alteration is at phonological level\n",
    "        - to a computer, an/a is _probabilistically determined_ rather than derived with rules.\n",
    "        - therefore, their difference is NOT minimal to a computer, not to be determined with a single \\[MASK\\]\n",
    "    - In the same vein, be careful with alteration of Korean case markers\n",
    "        - e.g. chelsunun casinul/cakilul saranghanta. -- lul\\~ul alteration makes the pair not minimal.\n",
    "- Some words are not \"registered\" in BERT\n",
    "    - To maximize the efficiency, BERT does something called \"byte-pair encoding\"\n",
    "    - i.e., some words are registered divided into smaller units (not morphemes)\n",
    "    - Check if the keyword is recognized as \\[UNK\\] (unknown) or not.\n",
    "    - Particulary with Korean -- many \"common\" words are seperated into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Score\n",
    "from Lin et al. (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_score(\"The scholar that published the paper has ever resigned the position\t0\t7\t4\", classification_model_eng, tokenizer_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "* Use attention_heatmap() for heatmap\n",
    "    * input should be in list ([ ])\n",
    "    * vis_opt is either 0, 1, or 2 and changes the shape of the heatmap\n",
    "* Use visualize_attention_head() for BertViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attention_heatmap([\"Students didn't do their syntax homework\"],\n",
    "                  mask_model_eng, tokenizer_eng, device, vis_opt=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention_head(mask_model_eng, tokenizer_eng, \"Students submitted their syntax homework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "### Boxplot\n",
    "* Format the provided template file with your result\n",
    "    * Add upto factor1 if 2by2\n",
    "        * leave factor2 empty\n",
    "    * Add upto factor2 if 2by2by2\n",
    "* Edit the boxplot_config.txt\n",
    "    * filepath = _filename_\n",
    "    * factor1 = _1st factor name_\n",
    "    * factor1_vals = _values of the 1st factor_\n",
    "    * factor2 = _2nd factor name_\n",
    "        * leave empty if 2by2\n",
    "    * factor2_vals = _values of the 2nd factor_\n",
    "        * leave empty if 2by2\n",
    "    * variables_value = _variable name_\n",
    "    * mask_vals = _keywords in the mask_\n",
    "    * notch = _True to add a notch_\n",
    "    * title = _Title of the plot_\n",
    "    * size = _22 for 2by2; 222 for 2by2by2_\n",
    "* The resulting plots should be saved at the directory\n",
    "* By default, it shows the result with the NPIs as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_box_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot\n",
    "\n",
    "* Format your data as like barplot_sample.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bar_plot(\"barplot_sample.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Lin, Y., Tan, Y. C., & Frank, R. (2019). Open Sesame: Getting Inside BERT's Linguistic Knowledge. arXiv preprint arXiv:1906.01698."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}